{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEZ0mR6n8Y4i"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoFoD28s8efK",
        "outputId": "d6650f5f-de2c-4083-c703-890034796e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'AMD_ML_Chromatin'...\n",
            "remote: Enumerating objects: 257, done.\u001b[K\n",
            "remote: Total 257 (delta 0), reused 0 (delta 0), pack-reused 257 (from 1)\u001b[K\n",
            "Receiving objects: 100% (257/257), 3.23 MiB | 15.19 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r AMD_ML_Chromatin/\n",
        "!git clone https://github.com/ed29rice/AMD_ML_Chromatin.git\n",
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb66ClFd8eh2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from tempfile import TemporaryDirectory\n",
        "import copy\n",
        "import time\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from torch.nn import functional as F\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewWX2zWE8eme"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import AMD_ML_Chromatin.TECSAS as TECSAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKya18tdwEnP"
      },
      "outputs": [],
      "source": [
        "NEXP = 15\n",
        "n_neigbors = 2\n",
        "n_predict = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZMpZuZ-BKNx",
        "outputId": "d2dfcc2f-cd4d-4d29-c7e9-87c00b4986b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([26138, 76]) torch.Size([13153, 76]) torch.Size([13153, 76])\n"
          ]
        }
      ],
      "source": [
        "with open('./inputs/cont_chrom_odd_HR_GM12878_hg19.pickle', 'rb') as handle:\n",
        "    data = pickle.load(handle)\n",
        "\n",
        "train_set,val_set,test_set,all_averages,loci_indx = data\n",
        "\n",
        "all_loci = np.array(list(range(len(all_averages[0]))))\n",
        "ntrain_loci = all_loci[loci_indx]\n",
        "nval_loci = all_loci[~loci_indx][::2]\n",
        "ntest_loci = all_loci[~loci_indx][1::2]\n",
        "train_data=torch.tensor(train_set,dtype=torch.float)\n",
        "val_data=torch.tensor(val_set,dtype=torch.float)\n",
        "test_data=torch.tensor(test_set,dtype=torch.float)\n",
        "print(train_data.size(),val_data.size(),test_data.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8MeXqW0CD_p",
        "outputId": "6d7a6507-10be-4096-f259-10342d7ce055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of batch: 522\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "nfeatures = train_data.size()[1]-(2*(n_predict-1)+1)\n",
        "ostates = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "nbatches = 50\n",
        "bptt = len(train_data)//nbatches\n",
        "print('Size of batch:',bptt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CPVDb7ZCLRL"
      },
      "outputs": [],
      "source": [
        "def train_model(model,ext,epochs,optimizer,scheduler):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_val_loss = float('inf')\n",
        "    best_train_loss = float('inf')\n",
        "    bsteps = 0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print('Epoch:'+str(epoch),end='\\r')\n",
        "        lr_e = scheduler.get_last_lr()[0]\n",
        "        epoch_start_time = time.time()\n",
        "        train_loss=train(model,optimizer)\n",
        "        elapsed = time.time() - epoch_start_time\n",
        "        val_loss = evaluate(model, val_data, type='val')\n",
        "        if train_loss < best_train_loss:\n",
        "            best_train_loss = train_loss\n",
        "            torch.save(model.state_dict(), './best_train_model_params_all'+ext+'.pt')\n",
        "            bsteps = 0\n",
        "        else:\n",
        "            bsteps=bsteps+1\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), './best_val_model_params_all'+ext+'.pt')\n",
        "        if bsteps>5:\n",
        "            scheduler.step()\n",
        "            bsteps=0\n",
        "\n",
        "def train(model: nn.Module, optimizer) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    src_mask = None\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    bs=list(range(nbatches))\n",
        "    np.random.shuffle(bs)\n",
        "    counter=0\n",
        "    for batch in bs:\n",
        "        counter+=1\n",
        "        i=batch\n",
        "        data, targets = get_batch(train_data, i, n_predict=n_predict)\n",
        "        seq_len = data.size(0)\n",
        "        output, output_tf = model(data, src_mask)\n",
        "        loss = 0\n",
        "        for n in range(2*(n_predict-1)+1):\n",
        "            loss += criterion(output[:,n], targets[:,n].type(torch.LongTensor).to(device))\n",
        "        data.detach()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        total_loss = total_loss + loss.item()\n",
        "    return total_loss\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor, type='test') -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = None\n",
        "    nbatches_eval=len(eval_data)//bptt\n",
        "    with torch.no_grad():\n",
        "        for batch in range(nbatches_eval):\n",
        "            data, targets = get_batch(eval_data, batch, n_predict=n_predict)\n",
        "            output, output_tf = model(data, src_mask)\n",
        "            for n in range(2*(n_predict-1)+1):\n",
        "                total_loss += criterion(output[:,n], targets[:,n].type(torch.LongTensor).to(device)).item()\n",
        "    return total_loss\n",
        "\n",
        "def results(model,ext):\n",
        "    k=0\n",
        "    f=0\n",
        "    p='best_val'\n",
        "    model.load_state_dict(torch.load('./'+p+'_model_params_all'+ext+'.pt'))\n",
        "    model.eval()\n",
        "    nbatches_eval=len(test_data)//bptt\n",
        "    l=[]\n",
        "    lt=[]\n",
        "    failed_inputs=[]\n",
        "    failed_targets=[]\n",
        "    failed_pred=[]\n",
        "    failed_loci=[]\n",
        "    suc_inputs=[]\n",
        "    suc_targets=[]\n",
        "    suc_pred=[]\n",
        "    suc_loci=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in range(nbatches_eval):\n",
        "            data, targets, batch_loci = get_batch_test(test_data, batch, n_predict=n_predict, ndxs = ntest_loci)\n",
        "            prediction=model(data,None)[0].argmax(dim=-1)[:,n_predict-1].cpu()\n",
        "            idx=prediction!=targets[:,n_predict-1].cpu()\n",
        "            failed_inputs.append(targets[idx,n_predict-1].cpu())\n",
        "            failed_targets.append(data[idx].cpu())\n",
        "            failed_pred.append(prediction[idx])\n",
        "            failed_loci.append(batch_loci[idx])\n",
        "            idx=prediction==targets[:,n_predict-1].cpu()\n",
        "            suc_inputs.append(targets[idx,n_predict-1].cpu())\n",
        "            suc_targets.append(data[idx].cpu())\n",
        "            suc_pred.append(prediction[idx])\n",
        "            suc_loci.append(batch_loci[idx])\n",
        "            l.append(prediction)\n",
        "            lt.append(targets[:,n_predict-1].cpu())\n",
        "    failed_inputs=np.concatenate(failed_inputs)\n",
        "    failed_pred=np.concatenate(failed_pred)\n",
        "    failed_targets=np.concatenate(failed_targets)\n",
        "    failed_loci=np.concatenate(failed_loci)\n",
        "    suc_inputs=np.concatenate(suc_inputs)\n",
        "    suc_pred=np.concatenate(suc_pred)\n",
        "    suc_targets=np.concatenate(suc_targets)\n",
        "    suc_loci=np.concatenate(suc_loci)\n",
        "    with open('test.npy', 'wb') as f:\n",
        "        for ppp in [failed_inputs,failed_pred,failed_targets,failed_loci,suc_inputs,suc_pred,suc_targets,suc_loci]:\n",
        "            np.save(f, ppp)\n",
        "        np.save(f,all_averages)\n",
        "        np.save(f,loci_indx)\n",
        "    l=np.concatenate(l)\n",
        "    lt=np.concatenate(lt)\n",
        "\n",
        "    conf_matrix_P=np.zeros((5,5))\n",
        "    int_types_Or=lt\n",
        "    types_pyME=l\n",
        "    for i in range(5):\n",
        "        idx1=(int_types_Or==i)\n",
        "        subav=int_types_Or[idx1]\n",
        "        subprd_P=types_pyME[idx1]\n",
        "        for k in range(len(subprd_P)):\n",
        "            for j in range(5):\n",
        "                conf_matrix_P[i,j]+=(subprd_P[k]==j)\n",
        "        conf_matrix_P[i,:]=np.round(conf_matrix_P[i,:]/np.sum(idx1),3)\n",
        "    return np.round(np.sum(l==lt)/len(l),3), conf_matrix_P, suc_loci, failed_loci\n",
        "\n",
        "def black_box_function(emsize_by_nhead, nhead, d_hid, nlayers, dropout, lr):\n",
        "    nhead=2*int(nhead)\n",
        "    emsize=int(int(emsize_by_nhead)*nhead)\n",
        "    d_hid=2*int(d_hid)\n",
        "    nlayers=int(nlayers)\n",
        "    print(emsize_by_nhead, nhead, d_hid, nlayers, dropout, lr)\n",
        "    train_data.size()[1]-(2*(n_predict-1)+1)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = TECSAS.TECSAS(1, emsize, nhead, d_hid, nlayers, nfeatures, 5, dropout).to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr,weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5)\n",
        "    train_model(model,'0',75,optimizer,scheduler)\n",
        "    acc,conf_matrix_P,suc_loci,failed_loci=results(model,'0')\n",
        "    return acc\n",
        "\n",
        "def get_batch(source: Tensor, i: int, n_predict: int) -> Tuple[Tensor, Tensor]:\n",
        "    data = source[i*bptt:(i+1)*bptt,2*(n_predict-1)+1:][:,:,np.newaxis]\n",
        "    target = source[i*bptt:(i+1)*bptt,:2*(n_predict-1)+1]\n",
        "    return data.to(device), target.to(device)\n",
        "\n",
        "def get_batch_test(source: Tensor, i: int, n_predict: int, ndxs ) -> Tuple[Tensor, Tensor]:\n",
        "    data = source[i*bptt:(i+1)*bptt,2*(n_predict-1)+1:][:,:,np.newaxis]\n",
        "    target = source[i*bptt:(i+1)*bptt,:2*(n_predict-1)+1]\n",
        "    indexes = ndxs[i*bptt:(i+1)*bptt]\n",
        "    return data.to(device), target.to(device), indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OWm86arDA3i"
      },
      "outputs": [],
      "source": [
        "# Bounded region of parameter space\n",
        "pbounds = {'emsize_by_nhead':(2,16), 'nhead':(1,4), 'd_hid':(1,64), 'nlayers':(1,4), 'dropout':(0.01,0.9), 'lr':(1e-2,2)}\n",
        "# Embedding dimension = emsize_by_head*2*nhead\n",
        "# Number of heads = 2*nhead\n",
        "# Hidden layer dimension = 2*d_hid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4fJYiLBB9Qn"
      },
      "outputs": [],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "optimizer_bo = BayesianOptimization(\n",
        "    f=black_box_function,\n",
        "    pbounds=pbounds,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvTqdyyoEI3i",
        "outputId": "dcfb8f0f-0234-45bb-99a9-ca247bbe5495"
      },
      "outputs": [],
      "source": [
        "optimizer_bo.set_gp_params(alpha=1e-2)\n",
        "optimizer_bo.maximize(init_points=5, n_iter=35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhJzi_3mGE7n",
        "outputId": "2a065337-9eb6-4cdc-ef2b-7c00ac815c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| iter | target | nlayers | d_hid | nhead | emsize... | lr | dropout |\n",
            "| 1 | 0.544 | 3 | 24 | 2 | 12 | 0.6077 | 0.8681 |\n",
            "| 2 | 0.601 | 3 | 120 | 2 | 18 | 1.835 | 0.323 |\n",
            "| 3 | 0.594 | 3 | 56 | 4 | 24 | 0.9001 | 0.433 |\n",
            "| 4 | 0.566 | 2 | 82 | 2 | 14 | 0.1487 | 0.2527 |\n",
            "| 5 | 0.579 | 2 | 54 | 6 | 12 | 0.9097 | 0.7364 |\n",
            "| 6 | 0.603 | 3 | 126 | 2 | 24 | 0.789 | 0.03329 |\n",
            "| 7 | 0.6 | 1 | 114 | 6 | 90 | 1.577 | 0.04214 |\n",
            "| 8 | 0.583 | 1 | 126 | 6 | 12 | 1.698 | 0.4483 |\n",
            "| 9 | 0.584 | 3 | 56 | 2 | 30 | 0.4765 | 0.306 |\n",
            "| 10 | 0.573 | 3 | 104 | 2 | 30 | 0.7702 | 0.4007 |\n",
            "| 11 | 0.557 | 1 | 122 | 6 | 72 | 0.1007 | 0.382 |\n",
            "| 12 | 0.588 | 2 | 58 | 4 | 8 | 0.5022 | 0.09446 |\n",
            "| 13 | 0.562 | 1 | 38 | 6 | 90 | 1.479 | 0.8044 |\n",
            "| 14 | 0.587 | 1 | 98 | 6 | 78 | 0.1926 | 0.4081 |\n",
            "| 15 | 0.581 | 2 | 110 | 2 | 24 | 1.439 | 0.2504 |\n",
            "| 16 | 0.568 | 3 | 114 | 4 | 16 | 1.053 | 0.6968 |\n",
            "| 17 | 0.58 | 3 | 56 | 2 | 30 | 0.515 | 0.3136 |\n",
            "| 18 | 0.55 | 3 | 126 | 2 | 26 | 1.824 | 0.7959 |\n",
            "| 19 | 0.588 | 3 | 118 | 2 | 16 | 1.678 | 0.3414 |\n",
            "| 20 | 0.573 | 1 | 114 | 6 | 90 | 1.963 | 0.1901 |\n",
            "| 21 | 0.546 | 3 | 126 | 2 | 26 | 0.2074 | 0.5626 |\n",
            "| 22 | 0.604 | 2 | 98 | 6 | 84 | 0.5084 | 0.2646 |\n",
            "| 23 | 0.565 | 3 | 96 | 2 | 6 | 1.364 | 0.3025 |\n",
            "| 24 | 0.602 | 2 | 104 | 4 | 36 | 0.7807 | 0.5439 |\n",
            "| 25 | 0.605 | 1 | 34 | 4 | 52 | 0.3557 | 0.6503 |\n",
            "| 26 | 0.562 | 2 | 56 | 6 | 24 | 1.08 | 0.5236 |\n",
            "| 27 | 0.592 | 2 | 98 | 6 | 78 | 0.6513 | 0.08031 |\n",
            "| 28 | 0.606 | 3 | 24 | 6 | 84 | 0.6283 | 0.3007 |\n",
            "| 29 | 0.593 | 2 | 104 | 4 | 36 | 0.6245 | 0.6283 |\n",
            "| 30 | 0.583 | 1 | 98 | 6 | 84 | 0.502 | 0.5674 |\n",
            "| 31 | 0.573 | 2 | 48 | 2 | 16 | 0.8638 | 0.577 |\n",
            "| 32 | 0.581 | 3 | 24 | 6 | 84 | 0.9185 | 0.6461 |\n",
            "| 33 | 0.584 | 3 | 24 | 6 | 84 | 0.7757 | 0.5703 |\n",
            "| 34 | 0.542 | 2 | 34 | 6 | 36 | 1.199 | 0.7958 |\n",
            "| 35 | 0.591 | 2 | 104 | 6 | 54 | 0.4303 | 0.2689 |\n"
          ]
        }
      ],
      "source": [
        "!awk '{print $1,$2,$1,$4,$1,$16,$1,$6,$1,$14,$1,$10,$1,$12,$1,$8,$1}' dummy2 | head -n1\n",
        "!awk '{print $1,$2,$1,$4,$1,int($16),$1,2*int($6),$1,2*int($14),$1,int($10)*2*int($14),$1,$12,$1,$8,$1}' dummy2 | tail -n+2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iterations = [0.544,0.601,0.594,0.566,0.579,0.603,0.6  ,0.583,0.584,0.573,0.557,0.588,0.562,0.587,0.581,0.568,0.58 ,0.55 ,0.588,0.573,0.546,0.604,0.565,0.602,0.605,0.562,0.592,0.606,0.593,0.583,0.573,0.581,0.584,0.542,0.591]\n",
        "plt.plot(iterations,'.-',markersize=10)\n",
        "plt.title('Bayesian Optimization Progress')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu-CKDGFisW_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
